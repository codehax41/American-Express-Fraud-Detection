{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n\nThe dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:\n\n- D_* = Delinquency variables\n- S_* = Spend variables\n- P_* = Payment variables\n- B_* = Balance variables\n- R_* = Risk variables\n\n**Task is to predict, for each customer_ID, the probability of a future payment default (target = 1).**\n\nNote: that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# visualization tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport os\nimport gc\nimport glob\nimport tqdm\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgbm\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n# set the warning off\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time\nimport cupy, cudf # GPU libraries\nimport matplotlib.pyplot as plt, gc, os\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# kaggle utils\n#import ../input/kaggle-utils/kaggle_utils.py as kaggle_utils\n#../input/kaggle-utils\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-06-30T15:14:42.835922Z","iopub.execute_input":"2022-06-30T15:14:42.836344Z","iopub.status.idle":"2022-06-30T15:14:47.585395Z","shell.execute_reply.started":"2022-06-30T15:14:42.836252Z","shell.execute_reply":"2022-06-30T15:14:47.584400Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"You can see that the data set consist of 16.39 GB for training data and 33.89 GB for test data. ","metadata":{}},{"cell_type":"code","source":"pd.set_option(\"display.max_columns\", None)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:14:47.588326Z","iopub.execute_input":"2022-06-30T15:14:47.588670Z","iopub.status.idle":"2022-06-30T15:14:47.593617Z","shell.execute_reply.started":"2022-06-30T15:14:47.588623Z","shell.execute_reply":"2022-06-30T15:14:47.592696Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Some functions for kaggle utils & amex metric","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/kyakovlev\n# https://www.kaggle.com/competitions/amex-default-prediction/discussion/327534\ndef amex_metric_mod(y_true, y_pred):\n\n    labels     = np.transpose(np.array([y_true, y_pred]))\n    labels     = labels[labels[:, 1].argsort()[::-1]]\n    weights    = np.where(labels[:,0]==0, 20, 1)\n    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n\n    gini = [0,0]\n    for i in [1,0]:\n        labels         = np.transpose(np.array([y_true, y_pred]))\n        labels         = labels[labels[:, i].argsort()[::-1]]\n        weight         = np.where(labels[:,0]==0, 20, 1)\n        weight_random  = np.cumsum(weight / np.sum(weight))\n        total_pos      = np.sum(labels[:, 0] *  weight)\n        cum_pos_found  = np.cumsum(labels[:, 0] * weight)\n        lorentz        = cum_pos_found / total_pos\n        gini[i]        = np.sum((lorentz - weight_random) * weight)\n\n    return 0.5 * (gini[1]/gini[0] + top_four)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-06-30T15:14:47.595036Z","iopub.execute_input":"2022-06-30T15:14:47.595608Z","iopub.status.idle":"2022-06-30T15:14:47.610150Z","shell.execute_reply.started":"2022-06-30T15:14:47.595573Z","shell.execute_reply":"2022-06-30T15:14:47.609237Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## About dataset: CSV, Parquet and Feather","metadata":{}},{"cell_type":"markdown","source":"- We cant directly import as csv due to memory restriction.\n- Both CSV and Parquet formats are used to store data, but they can't be any more different internally. CSVs are what you call row storage, while Parquet files organize the data in columns. In a nutshell, column storage files are more lightweight, as adequate compression can be done for each column\n- Also provide Low storage consumption. \n\n- Feather is a fast, lightweight, and easy-to-use binary file format for storing data frames. It has a few specific design goals: Lightweight, minimal API: make pushing data frames in and out of memory as simple as possible. Language agnostic: Feather files are the same whether written by Python or R code.","metadata":{}},{"cell_type":"code","source":"#Reading Data as Parquet\ntrain_df = pd.read_parquet('../input/amex-data-integer-dtypes-parquet-format/train.parquet').groupby('customer_ID').tail(2).set_index('customer_ID', drop=True).sort_index()\ntrain_labels = pd.read_csv('../input/amex-default-prediction/train_labels.csv').set_index('customer_ID', drop=True).sort_index()\ntrain_df = pd.merge(train_df, train_labels, left_index=True, right_index=True)\ntest_df = pd.read_parquet('../input/amex-data-integer-dtypes-parquet-format/test.parquet').groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:03:42.072677Z","iopub.execute_input":"2022-06-30T08:03:42.073317Z","iopub.status.idle":"2022-06-30T08:05:12.288406Z","shell.execute_reply.started":"2022-06-30T08:03:42.073276Z","shell.execute_reply":"2022-06-30T08:05:12.283019Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:07:16.340826Z","iopub.execute_input":"2022-06-30T08:07:16.341445Z","iopub.status.idle":"2022-06-30T08:07:16.398095Z","shell.execute_reply.started":"2022-06-30T08:07:16.341404Z","shell.execute_reply":"2022-06-30T08:07:16.395969Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# **Some Stats & EDA**","metadata":{}},{"cell_type":"code","source":"# all_cols = train_df.columns\n# #cat_cols\n# non_use_cols = ['S_2','B_30','B_38','D_114','D_116','D_117','D_120','D_126','D_63','D_64','D_66','D_68', 'target']\n# feature_cols = [col for col in all_cols if col not in non_use_cols]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:23:34.170134Z","iopub.execute_input":"2022-06-30T12:23:34.170522Z","iopub.status.idle":"2022-06-30T12:23:34.175401Z","shell.execute_reply.started":"2022-06-30T12:23:34.170486Z","shell.execute_reply":"2022-06-30T12:23:34.174172Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"feat_Delinquency = [c for c in train_df.columns if c.startswith('D_')]\nfeat_Spend = [c for c in train_df.columns if c.startswith('S_')]\nfeat_Payment = [c for c in train_df.columns if c.startswith('P_')]\nfeat_Balance = [c for c in train_df.columns if c.startswith('B_')]\nfeat_Risk = [c for c in train_df.columns if c.startswith('R_')]\nprint(f'Total number of Delinquency variables: {len(feat_Delinquency)}')\nprint(f'Total number of Spend variables: {len(feat_Spend)}')\nprint(f'Total number of Payment variables: {len(feat_Payment)}')\nprint(f'Total number of Balance variables: {len(feat_Balance)}')\nprint(f'Total number of Risk variables: {len(feat_Risk)}')\nlabels=['Delinquency', 'Spend','Payment','Balance','Risk']\nvalues= [len(feat_Delinquency), len(feat_Spend),len(feat_Payment), len(feat_Balance),len(feat_Risk)]\nfig_1 = go.Figure()\nfig_1.add_trace(go.Pie(values = values,labels = labels,hole = 0.6, \n                     hoverinfo ='label+percent'))\nfig_1.update_traces(textfont_size = 12, hoverinfo ='label+percent',textinfo ='label', \n                  showlegend = False,marker = dict(colors =[\"#70d6ff\",\"#ff9770\"]),\n                  title = dict(text = 'Feature Distribution'))  \nfig_1.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:18:32.821928Z","iopub.execute_input":"2022-06-30T08:18:32.822539Z","iopub.status.idle":"2022-06-30T08:18:32.892963Z","shell.execute_reply.started":"2022-06-30T08:18:32.822475Z","shell.execute_reply":"2022-06-30T08:18:32.891694Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Basic Level Analysis","metadata":{}},{"cell_type":"code","source":"categorical_col = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\ndescrete_cols=['B_30', 'B_38', 'D_63', 'D_64', 'D_66', 'D_68',\n          'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'target']\n#all categorial columns are stored in categorical_col\ncategorical_col.extend(descrete_cols)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:56:09.484383Z","iopub.status.idle":"2022-06-30T08:56:09.484802Z","shell.execute_reply.started":"2022-06-30T08:56:09.484598Z","shell.execute_reply":"2022-06-30T08:56:09.484619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #EDA from\n# #https://www.kaggle.com/code/duanchenliu/data-exploring-dl/notebook\ntarget_col = 'target'\n#Target Distribution\ncount = train_df[target_col].value_counts()\nprint(count)\nprint(\"percentage of not default --- >\",count[0]/train_df.shape[0])\nprint(\"percentage of default --->\", count[1]/train_df.shape[0])\nfig = go.Figure()\nfig.add_trace(go.Bar(x= ['Paid', \"Default\"],y=count.values,\n                     marker_color = ['#9900cc','#ffff80'],\n                     text = [str(round(count[0]/train_df.shape[0],2) * 100) + '%' , str(round(count[1]/train_df.shape[0], 2) * 100) + '%']))\nfig.update_layout(template = 'plotly_dark',\n                  title = \"target value distribution\",\n                  width = 500,\n                  height = 500)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:11:22.399270Z","iopub.execute_input":"2022-06-30T08:11:22.399670Z","iopub.status.idle":"2022-06-30T08:11:23.191042Z","shell.execute_reply.started":"2022-06-30T08:11:22.399636Z","shell.execute_reply":"2022-06-30T08:11:23.189979Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### correlation with target","metadata":{}},{"cell_type":"code","source":"#col = [c for c in data.columns if data[c].dtypes != 'object']\ncorr = train_df.corrwith(train_df[target_col], axis=0)\nval = [str(round(v ,2) *100) + '%' for v in corr.values]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(y=corr.index, x= corr.values,\n                     orientation='h',\n                     marker_color = '#9900cc',\n                     text = val,\n                     textposition = 'outside',\n                     textfont_color = '#ffff80'))\nfig.update_layout(template = 'plotly_dark',\n                  title = \"Correlation with Target\",\n                  width = 800,\n                  height = 3000)\nfig.update_xaxes(range=[-2,2])\n\n# negative correlation top 5: P_2 -67%, B_2 -56%, B_18 -55%, B_33 -52%, D_62 -37% \n# postive correlation top 5: B_9 54%, D_55 54%, D_44 53%, D_61 53%, B_3 51%","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:15:14.731561Z","iopub.execute_input":"2022-06-30T08:15:14.732237Z","iopub.status.idle":"2022-06-30T08:15:17.839072Z","shell.execute_reply.started":"2022-06-30T08:15:14.732201Z","shell.execute_reply":"2022-06-30T08:15:17.838112Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# NAN_VALUE = -127\n# train = train.fillna(NAN_VALUE) \n# print('shape of data:', train.shape)\n\n# def process_and_feature_engineer(df):\n#     # FEATURE ENGINEERING FROM \n#     # https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n#     all_cols = [c for c in list(df.columns) if c not in ['customer_ID','S_2']]\n#     cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n#     num_features = [col for col in all_cols if col not in cat_features]\n\n#     test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n#     test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n\n#     test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n#     test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n\n#     df = cudf.concat([test_num_agg, test_cat_agg], axis=1)\n#     del test_num_agg, test_cat_agg\n#     print('shape after engineering', df.shape )\n    \n#     return df\n\n# train = process_and_feature_engineer(train)","metadata":{"execution":{"iopub.execute_input":"2022-06-26T07:20:21.570526Z","iopub.status.busy":"2022-06-26T07:20:21.570148Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"~## Above code : System getting stopped due to huge data so we will import either the agg. dataset~","metadata":{}},{"cell_type":"code","source":"# #aggregated data, min max avg count etc, here the target label is already added\n# #link https://www.kaggle.com/datasets/huseyincot/amex-agg-data-pickle\n# train = pd.read_pickle(\"../input/amex-agg-data-pickle/train_agg.pkl\", compression=\"gzip\")\n# test = pd.read_pickle(\"../input/amex-agg-data-pickle/test_agg.pkl\", compression=\"gzip\")","metadata":{"execution":{"iopub.execute_input":"2022-06-27T14:42:53.047396Z","iopub.status.busy":"2022-06-27T14:42:53.046585Z","iopub.status.idle":"2022-06-27T14:42:53.051987Z","shell.execute_reply":"2022-06-27T14:42:53.050796Z","shell.execute_reply.started":"2022-06-27T14:42:53.04736Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing Data","metadata":{}},{"cell_type":"code","source":"#Custom Color Palette 🎨\ncustom_colors = [\"#ffd670\",\"#70d6ff\",\"#ff4d6d\",\"#8338ec\",\"#90cf8e\"]\ncustomPalette = sns.set_palette(sns.color_palette(custom_colors))\nsns.palplot(sns.color_palette(custom_colors),size=1.2)\nplt.tick_params(axis='both', labelsize=0, length = 0)\nbackground_color = 'white'\nmissing = pd.DataFrame(columns = ['% Missing values'],data = train_df.isnull().sum()/len(train_df))\nfig = plt.figure(figsize = (20, 60),facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace = 0.5, hspace = 0.5)\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\",\"bottom\",\"left\"]:\n    ax0.spines[s].set_visible(False)\nsns.heatmap(missing,cbar = False,annot = True,fmt =\".2%\", linewidths = 2,cmap = custom_colors,vmax = 1, ax = ax0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:19:29.269032Z","iopub.execute_input":"2022-06-30T08:19:29.269658Z","iopub.status.idle":"2022-06-30T08:19:32.895227Z","shell.execute_reply.started":"2022-06-30T08:19:29.269619Z","shell.execute_reply":"2022-06-30T08:19:32.894284Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\ndel_cols = [c for c in train_df.columns if (c.startswith(('D','t'))) & (c not in cat_cols)]\ndf_del = train_df[del_cols]\nspd_cols = [c for c in train_df.columns if (c.startswith(('S','t'))) & (c not in cat_cols)]\ndf_spd = train_df[spd_cols]\npay_cols = [c for c in train_df.columns if (c.startswith(('P','t'))) & (c not in cat_cols)]\ndf_pay = train_df[pay_cols]\nbal_cols = [c for c in train_df.columns if (c.startswith(('B','t'))) & (c not in cat_cols)]\ndf_bal = train_df[bal_cols]\nris_cols = [c for c in train_df.columns if (c.startswith(('R','t'))) & (c not in cat_cols)]\ndf_ris = train_df[ris_cols]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:21:42.348561Z","iopub.execute_input":"2022-06-30T08:21:42.349706Z","iopub.status.idle":"2022-06-30T08:21:42.705825Z","shell.execute_reply.started":"2022-06-30T08:21:42.349657Z","shell.execute_reply":"2022-06-30T08:21:42.704732Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize =(11,11))\ncorr = df_del.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\nsns.heatmap(corr, mask = mask, robust = True, center = 0,square = True, linewidths =.6, cmap = custom_colors)\nplt.title('Correlation of Delinquency Variables')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:19:46.017904Z","iopub.execute_input":"2022-06-30T08:19:46.018588Z","iopub.status.idle":"2022-06-30T08:20:02.526236Z","shell.execute_reply.started":"2022-06-30T08:19:46.018547Z","shell.execute_reply":"2022-06-30T08:20:02.525268Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# #https://www.kaggle.com/code/devsubhash/amex-eda-default-prediction\n# fig, axes = plt.subplots(8, 3, figsize = (16,18))\n# fig.suptitle('Distribution of Spend Variables', fontsize = 15, x = 0.5, y = 1)\n# for i, ax in enumerate(axes.reshape(-1)):\n#     if i < len(spd_cols) - 1:\n#         sns.kdeplot(x = spd_cols[i], hue ='target', data = df_spd, fill = True, ax = ax, palette =[\"#e63946\",\"#8338ec\"])\n#         ax.tick_params()\n#         ax.xaxis.get_label()\n#         ax.set_ylabel('')\n# plt.tight_layout()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:21:00.607610Z","iopub.execute_input":"2022-06-30T08:21:00.608694Z","iopub.status.idle":"2022-06-30T08:21:00.613398Z","shell.execute_reply.started":"2022-06-30T08:21:00.608647Z","shell.execute_reply":"2022-06-30T08:21:00.612251Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (11,11))\ncorr = df_spd.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask = mask, robust = True, center = 0,square = True, linewidths = .6, cmap = custom_colors)\nplt.title('Correlation of Spend Variables')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:22:04.724640Z","iopub.execute_input":"2022-06-30T08:22:04.725006Z","iopub.status.idle":"2022-06-30T08:22:06.512403Z","shell.execute_reply.started":"2022-06-30T08:22:04.724976Z","shell.execute_reply":"2022-06-30T08:22:06.511459Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (6,6))\ncorr = df_pay.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\nsns.heatmap(corr, mask = mask, robust = True, center = 0,square = True, linewidths = .6, cmap = custom_colors)\nplt.title('Correlation of Payment Variables')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:22:25.090167Z","iopub.execute_input":"2022-06-30T08:22:25.090599Z","iopub.status.idle":"2022-06-30T08:22:25.401092Z","shell.execute_reply.started":"2022-06-30T08:22:25.090561Z","shell.execute_reply":"2022-06-30T08:22:25.400033Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (11,11))\ncorr = df_bal.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\nsns.heatmap(corr, mask = mask, robust=True, center = 0,square = True, linewidths =.6, cmap = custom_colors)\nplt.title('Correlation of Balance Variables')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:22:38.513047Z","iopub.execute_input":"2022-06-30T08:22:38.513407Z","iopub.status.idle":"2022-06-30T08:22:42.926467Z","shell.execute_reply.started":"2022-06-30T08:22:38.513376Z","shell.execute_reply":"2022-06-30T08:22:42.925489Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(11,11))\ncorr = df_ris.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask = mask, robust = True, center = 0, square = True, linewidths =.6, cmap = custom_colors)\nplt.title('Correlation of Risk Variables')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:22:43.859115Z","iopub.execute_input":"2022-06-30T08:22:43.860194Z","iopub.status.idle":"2022-06-30T08:22:46.608263Z","shell.execute_reply.started":"2022-06-30T08:22:43.860149Z","shell.execute_reply":"2022-06-30T08:22:46.607112Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"There are several strong correlations with the target variable. Payment 2 is the most negatively correlated with the probability of defaulting with a correlation of -0.67, while Delinquency 48 is the most positively correlated overall at 0.61. Delinquency 87 is also missing from the correlations above due to the proportion of null values. In fact, 24 of the top 30 features with missing values are in Delinquency variables.","metadata":{}},{"cell_type":"code","source":"#Removing outlier cols\noutlier_list = []\noutlier_col = []\nfor col in feature_cols :\n    temp_df = train_df[(train_df[col] > train_df[col].mean() + train_df[col].std() * 200) |\n                       (train_df[col] < train_df[col].mean() - train_df[col].std() * 200) ]\n    if len(temp_df) >0 and len(temp_df) <6 : \n        outliers = temp_df.index.to_list()\n        outlier_list.extend(outliers)\n        outlier_col.append(col)\n        #print(col, len(temp_df))\noutlier_list = list(set(outlier_list))\ntrain_df.drop(outlier_list, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:30.601033Z","iopub.execute_input":"2022-06-30T08:23:30.601396Z","iopub.status.idle":"2022-06-30T08:23:35.511422Z","shell.execute_reply.started":"2022-06-30T08:23:30.601366Z","shell.execute_reply":"2022-06-30T08:23:35.510408Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"y = train_df['target'].copy()\nx = train_df[feature_cols]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:35.513310Z","iopub.execute_input":"2022-06-30T08:23:35.513680Z","iopub.status.idle":"2022-06-30T08:23:35.679311Z","shell.execute_reply.started":"2022-06-30T08:23:35.513651Z","shell.execute_reply":"2022-06-30T08:23:35.678294Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"markdown","source":"## 1. LGBM: Acc as per AMEX Metric - 78.6%","metadata":{}},{"cell_type":"code","source":"#Cross validation (KFold = 3)\nkf = KFold(n_splits = 4)\nmodels = []\nlgbm_params ={\"objective\":\"binary\",\n              \"random_seed\":1234}\n\nfor train_index, val_index in kf.split(x):\n    X_train = x.iloc[train_index]\n    X_valid = x.iloc[val_index]\n    Y_train = y.iloc[train_index]\n    Y_valid = y.iloc[val_index]\n    \n    lgbm_train = lgbm.Dataset(X_train, Y_train)\n    lgbm_eval = lgbm.Dataset(X_valid, Y_valid, reference=lgbm_train)\n    \n    model_lgbm = lgbm.train(lgbm_params,\n                           lgbm_train,\n                           valid_sets = lgbm_eval,\n                           num_boost_round = 300,\n                           early_stopping_rounds = 20,\n                           verbose_eval = 10,\n                           )\n    y_pred = model_lgbm.predict(X_valid, num_iteration = model_lgbm.best_iteration)\n        \n    print (accuracy_score(Y_valid, np.round(y_pred)))\n    \n    models.append(model_lgbm)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:24:13.274630Z","iopub.execute_input":"2022-06-30T08:24:13.275004Z","iopub.status.idle":"2022-06-30T08:30:26.621803Z","shell.execute_reply.started":"2022-06-30T08:24:13.274974Z","shell.execute_reply":"2022-06-30T08:30:26.620757Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#test_df = pd.read_parquet('../input/amex-data-integer-dtypes-parquet-format/test.parquet').groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()\ntest_df = test_df[feature_cols]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:30:26.623911Z","iopub.execute_input":"2022-06-30T08:30:26.624385Z","iopub.status.idle":"2022-06-30T08:30:26.784928Z","shell.execute_reply.started":"2022-06-30T08:30:26.624345Z","shell.execute_reply":"2022-06-30T08:30:26.783958Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:30:26.786458Z","iopub.execute_input":"2022-06-30T08:30:26.786855Z","iopub.status.idle":"2022-06-30T08:30:27.081529Z","shell.execute_reply.started":"2022-06-30T08:30:26.786818Z","shell.execute_reply":"2022-06-30T08:30:27.080303Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"preds = []\nfor model in models:\n    pred = model.predict(test_df)\n    preds.append(pred)\n    \npreds_array = np.array(preds)\npreds_mean = np.mean(preds_array, axis =0)\n#Submission\nsub = pd.read_csv('../input/amex-default-prediction/sample_submission.csv')\nsub[\"prediction\"] = preds_mean\nsub.to_csv('submission_lgbm.csv', index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:33:39.577713Z","iopub.execute_input":"2022-06-30T08:33:39.578232Z","iopub.status.idle":"2022-06-30T08:34:46.853279Z","shell.execute_reply.started":"2022-06-30T08:33:39.578188Z","shell.execute_reply":"2022-06-30T08:34:46.852148Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:34:46.856773Z","iopub.execute_input":"2022-06-30T08:34:46.857124Z","iopub.status.idle":"2022-06-30T08:34:47.185994Z","shell.execute_reply.started":"2022-06-30T08:34:46.857094Z","shell.execute_reply":"2022-06-30T08:34:47.184694Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## 2. CatBoostClassifier: Acc as per AMEX Metric - 78.6%","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_feather('../input/amexfeather/train_data.ftr')\ntrain_df = train_df.groupby('customer_ID').tail(1).set_index('customer_ID')\n\ntest_df = pd.read_feather('../input/amexfeather/test_data.ftr')\ntest_df = test_df.groupby('customer_ID').tail(1).set_index('customer_ID')\n\ndf_subm = pd.read_csv(\"../input/amex-default-prediction/sample_submission.csv\")\n\nall_cols = train_df.columns\n#cat_cols\nnon_use_cols = ['S_2','B_30','B_38','D_114','D_116','D_117','D_120','D_126','D_63','D_64','D_66','D_68', 'target']\nfeature_cols = [col for col in all_cols if col not in non_use_cols]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:38:38.937477Z","iopub.execute_input":"2022-06-30T08:38:38.938215Z","iopub.status.idle":"2022-06-30T08:39:50.970396Z","shell.execute_reply.started":"2022-06-30T08:38:38.938180Z","shell.execute_reply":"2022-06-30T08:39:50.969400Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Removing outlier cols\noutlier_list = []\noutlier_col = []\nfor col in feature_cols :\n    temp_df = train_df[(train_df[col] > train_df[col].mean() + train_df[col].std() * 200) |\n                       (train_df[col] < train_df[col].mean() - train_df[col].std() * 200) ]\n    if len(temp_df) >0 and len(temp_df) <6 : \n        outliers = temp_df.index.to_list()\n        outlier_list.extend(outliers)\n        outlier_col.append(col)\n        #print(col, len(temp_df))\noutlier_list = list(set(outlier_list))\ntrain_df.drop(outlier_list, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:39:50.972105Z","iopub.execute_input":"2022-06-30T08:39:50.972433Z","iopub.status.idle":"2022-06-30T08:40:57.715804Z","shell.execute_reply.started":"2022-06-30T08:39:50.972398Z","shell.execute_reply":"2022-06-30T08:40:57.714824Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n\nlab_enc = LabelEncoder()\nfor cat_feat in cat_cols:\n    train_df[cat_feat] = lab_enc.fit_transform(train_df[cat_feat])\n    test_df[cat_feat] = lab_enc.transform(test_df[cat_feat])\n# define dataset\nX = train_df.drop('target', axis=1)\ny = train_df['target']\n\n# creating dataset split for prediction\nX_train, X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=42) # 80-20 split\n\nclf = CatBoostClassifier(iterations = 3001, random_state = 42, nan_mode ='Min',task_type =\"GPU\")\nclf.fit(X_train, y_train, eval_set = [(X_test, y_test)], cat_features=cat_cols,  verbose = 100)\npreds = clf.predict_proba(X_test)[:, 1]\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:40:57.717240Z","iopub.execute_input":"2022-06-30T08:40:57.717576Z","iopub.status.idle":"2022-06-30T08:45:52.455010Z","shell.execute_reply.started":"2022-06-30T08:40:57.717543Z","shell.execute_reply":"2022-06-30T08:45:52.453856Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"y_preds = clf.predict_proba(test_df)[:, 1]\ndf_subm[\"prediction\"] = y_preds\ndf_subm.to_csv('submission_catb.csv', index=False)\ndf_subm","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:45:52.457194Z","iopub.execute_input":"2022-06-30T08:45:52.457559Z","iopub.status.idle":"2022-06-30T08:47:13.769365Z","shell.execute_reply.started":"2022-06-30T08:45:52.457524Z","shell.execute_reply":"2022-06-30T08:47:13.768309Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## 3: Didn't work because LogR. dont work with NaN","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_feather('../input/amexfeather/train_data.ftr')\ntest_df = pd.read_feather('../input/amexfeather/test_data.ftr')\ndf_subm = pd.read_csv(\"../input/amex-default-prediction/sample_submission.csv\")\n\ngc.collect()\n\ntrain_df = train_df.groupby('customer_ID').tail(1).set_index('customer_ID')\ntest_df = test_df.groupby('customer_ID').tail(1).set_index('customer_ID')\n\ndel test_df['S_2']\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"titles=['Delinquency '+str(i).split('_')[1] if i.startswith('D') else 'Spend '+str(i).split('_')[1] \n        if i.startswith('S') else 'Payment '+str(i).split('_')[1]  if i.startswith('P') \n        else 'Balance '+str(i).split('_')[1] if i.startswith('B') else \n        'Risk '+str(i).split('_')[1] for i in train_df.columns[:-1]]\ncat_cols=['Balance 30', 'Balance 38', 'Delinquency 63', 'Delinquency 64', 'Delinquency 66', 'Delinquency 68',\n          'Delinquency 114', 'Delinquency 116', 'Delinquency 117', 'Delinquency 120', 'Delinquency 126', 'Target']\ntest_df.columns=titles[1:]\ntitles.append('Target')\ntrain_df.columns=titles\n\ndf = pd.read_feather('../input/amexfeather/train_data.ftr')\ndf_cat = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126','D_63','D_64', 'D_66', 'D_68'] \n\ndf_all = list(df.columns)\ndf_all.remove(\"customer_ID\")\ndf_all.remove(\"S_2\")\ndf_all.remove(\"D_142\")\n\n#finding set of numerical features by cosnducting simple set operations\ndf_num = list(set(df_all) - set(df_cat))\ndf = df[df_all]\nperc = 20.0 # Like N %\nmin_count =  int(((100-perc)/100)*df.shape[0] + 1)\ndf = df.dropna( axis=1, \n                thresh=min_count)\ndf=df.dropna()\ndf=df.reset_index()\ndf=df.drop(\"index\",axis=1)\n\ndf_all = list(df.columns)\n\ndf_num = list(set(df_all) - set(df_cat))\n\ndf_cat = list(set(df_all) - set(df_num))\ndf_all=list(df.columns)\ndf_all.remove(\"target\")\ndf_encoded = pd.get_dummies( df[df_all], \n                                        columns = df_cat,\n                                        drop_first = True )\nX = df_encoded\nY = df['target']\ntrain_X, test_X, train_y, test_y = train_test_split( X,\n                                                    Y,\n                                                    test_size = 0.3,\n                                                    random_state = 42 )\nsc = StandardScaler()\ntrain_X = sc.fit_transform(train_X)\ntest_X = sc.transform(test_X)\nlogit = LogisticRegression()\nlogit.fit( train_X, train_y)\npred_y = logit.predict(test_X)","metadata":{"execution":{"iopub.execute_input":"2022-06-29T02:18:52.853575Z","iopub.status.busy":"2022-06-29T02:18:52.853228Z","iopub.status.idle":"2022-06-29T02:19:50.037595Z","shell.execute_reply":"2022-06-29T02:19:50.036623Z","shell.execute_reply.started":"2022-06-29T02:18:52.853546Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(test_y,pred_y)\nprint(cm)\naccuracy_score(test_y, pred_y)\n\ngc.collect()\n\ny_preds = logit.predict_proba(test_df)[:, 1]\ndf_subm[\"prediction\"] = y_preds\ndf_subm.to_csv('submission_LR.csv', index=False)\ndf_subm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4th XGB: Mixed Feature - Acc as per AMEX Metric - 79.4%","metadata":{}},{"cell_type":"markdown","source":"Mixed Feature Means for every columns we have we have added min, max, avg, std, last because for few reason which is defaulter can be identifiable using these Mixed features as per quantile analysis.\n\nNeed of aggregated columns as features because:\nhttps://www.kaggle.com/code/narendra/timeseries-analysis-with-quantiles","metadata":{}},{"cell_type":"code","source":"print('RAPIDS version',cudf.__version__)\n#https://www.kaggle.com/code/sietseschrder/xgboost-starter-0-793\n# VERSION NAME FOR SAVED MODEL FILES\nVER = 1\n# TRAIN RANDOM SEED\nSEED = 42\n# FILL NAN VALUE\nNAN_VALUE = -127 # will fit in int8\n# FOLDS PER MODEL\nFOLDS = 5\ndef read_file(path = '', usecols = None):\n    # LOAD DATAFRAME\n    if usecols is not None: df = cudf.read_parquet(path, columns=usecols)\n    else: df = cudf.read_parquet(path)\n    # REDUCE DTYPE FOR CUSTOMER AND DATE\n    df['customer_ID'] = df['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\n    df.S_2 = cudf.to_datetime( df.S_2 )\n    # SORT BY CUSTOMER AND DATE (so agg('last') works correctly)\n    df = df.fillna(-127)\n    return df\nprint('Reading train data...')\nTRAIN_PATH = '../input/amex-data-integer-dtypes-parquet-format/train.parquet'\ntrain = read_file(path = TRAIN_PATH)\n\ndef process_and_feature_engineer(df):\n    # FEATURE ENGINEERING FROM \n    # https://www.kaggle.com/code/huseyincot/amex-agg-data-how-it-created\n    all_cols = [c for c in list(df.columns) if c not in ['customer_ID','S_2']]\n    cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n    num_features = [col for col in all_cols if col not in cat_features]\n\n#     df.groupby(\"customer_ID\").fillna(\"backfill\")\n    test_num_agg = df.groupby(\"customer_ID\")[num_features].agg(['mean', 'std', 'min', 'max', 'last'])\n    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n\n    test_cat_agg = df.groupby(\"customer_ID\")[cat_features].agg(['count', 'last', 'nunique'])\n    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n\n    df = cudf.concat([test_num_agg, test_cat_agg], axis=1)\n    del test_num_agg, test_cat_agg\n    print('shape after engineering', df.shape )\n    \n    return df\n\ntrain = process_and_feature_engineer(train)\n\n# ADD TARGETS\ntargets = cudf.read_csv('../input/amex-default-prediction/train_labels.csv')\ntargets['customer_ID'] = targets['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\ntargets = targets.set_index('customer_ID')\ntrain = train.merge(targets, left_index=True, right_index=True, how='left')\ntrain.target = train.target.astype('int8')\ndel targets\n\n# NEEDED TO MAKE CV DETERMINISTIC (cudf merge above randomly shuffles rows)\ntrain = train.sort_index().reset_index()\n\n# FEATURES\nFEATURES = train.columns[1:-1]\nprint(f'There are {len(FEATURES)} features!')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T10:47:36.511284Z","iopub.execute_input":"2022-06-30T10:47:36.512177Z","iopub.status.idle":"2022-06-30T10:48:04.285308Z","shell.execute_reply.started":"2022-06-30T10:47:36.512127Z","shell.execute_reply":"2022-06-30T10:48:04.284214Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# NEEDED WITH DeviceQuantileDMatrix BELOW\nclass IterLoadForDMatrix(xgb.core.DataIter):\n    def __init__(self, df=None, features=None, target=None, batch_size=256*1024):\n        self.features = features\n        self.target = target\n        self.df = df\n        self.it = 0 # set iterator to 0\n        self.batch_size = batch_size\n        self.batches = int( np.ceil( len(df) / self.batch_size ) )\n        super().__init__()\n\n    def reset(self):\n        '''Reset the iterator'''\n        self.it = 0\n\n    def next(self, input_data):\n        '''Yield next batch of data.'''\n        if self.it == self.batches:\n            return 0 # Return 0 when there's no more batch.\n        \n        a = self.it * self.batch_size\n        b = min( (self.it + 1) * self.batch_size, len(self.df) )\n        dt = cudf.DataFrame(self.df.iloc[a:b])\n        input_data(data=dt[self.features], label=dt[self.target]) #, weight=dt['weight'])\n        self.it += 1\n        return 1\n    \ndef xgboost_amex_metric_mod(predt: np.ndarray, dtrain: xgb.DMatrix):\n    y = dtrain.get_label()\n    return 'AMEXcustom', 1 - amex_metric_mod(y, predt)\n# LOAD XGB LIBRARY\n\nprint('XGB Version',xgb.__version__)\n\n# XGB MODEL PARAMETERS\nxgb_parms = { \n    'max_depth':4, \n    'learning_rate':0.03, \n    'subsample':0.8,\n    'colsample_bytree':0.6, \n    'objective':'binary:logistic',\n    'tree_method':'gpu_hist',\n    'predictor':'gpu_predictor',\n    'random_state':SEED\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-30T10:48:04.287634Z","iopub.execute_input":"2022-06-30T10:48:04.287980Z","iopub.status.idle":"2022-06-30T10:48:04.300628Z","shell.execute_reply.started":"2022-06-30T10:48:04.287945Z","shell.execute_reply":"2022-06-30T10:48:04.299156Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"importances = []\noof = []\ntrain = train.to_pandas() # free GPU memory\nTRAIN_SUBSAMPLE = 1.0\ngc.collect()\nfold_model_scores = []\n\nskf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\nfor fold,(train_idx, valid_idx) in enumerate(skf.split(\n            train, train.target )):\n    \n    # TRAIN WITH SUBSAMPLE OF TRAIN FOLD DATA\n    if TRAIN_SUBSAMPLE<1.0:\n        np.random.seed(SEED)\n        train_idx = np.random.choice(train_idx, \n                       int(len(train_idx)*TRAIN_SUBSAMPLE), replace=False)\n        np.random.seed(None)\n    \n    print('#'*25)\n    print('### Fold',fold+1)\n    print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n    print(f'### Training with {int(TRAIN_SUBSAMPLE*100)}% fold data...')\n    print('#'*25)\n    \n    # TRAIN, VALID, TEST FOR FOLD K\n    Xy_train = IterLoadForDMatrix(train.loc[train_idx], FEATURES, 'target')\n    X_valid = train.loc[valid_idx, FEATURES]\n    y_valid = train.loc[valid_idx, 'target']\n    \n    dtrain = xgb.DeviceQuantileDMatrix(Xy_train, max_bin=256)\n    dvalid = xgb.DMatrix(data=X_valid, label=y_valid)\n    \n    # TRAIN MODEL FOLD K\n    model = xgb.train(xgb_parms, \n                dtrain=dtrain,\n                evals=[(dtrain,'train'),(dvalid,'valid')],\n                custom_metric=xgboost_amex_metric_mod,\n                num_boost_round=9999,\n                early_stopping_rounds=1000,\n                verbose_eval=100) \n    model.save_model(f'XGB_v{VER}_fold{fold}.xgb')\n    \n    # GET FEATURE IMPORTANCE FOR FOLD K\n    dd = model.get_score(importance_type='weight')\n    df = pd.DataFrame({'feature':dd.keys(),f'importance_{fold}':dd.values()})\n    importances.append(df)\n            \n    # INFER OOF FOLD K\n    oof_preds = model.predict(dvalid)\n    acc = amex_metric_mod(y_valid.values, oof_preds)\n    fold_model_scores.append(acc)\n    print('Kaggle Metric =',acc,'\\n')\n    \n    # SAVE OOF\n    df = train.loc[valid_idx, ['customer_ID','target'] ].copy()\n    df['oof_pred'] = oof_preds\n    print(df.head())\n    oof.append( df )\n    \n    del dtrain, Xy_train, dd, df\n    del X_valid, y_valid, dvalid, model\n    _ = gc.collect()\n    \nprint('#'*25)\noof = pd.concat(oof,axis=0,ignore_index=True).set_index('customer_ID')\n\nprint(oof.head())\nprint(len(oof))\nacc = amex_metric_mod(oof.target.values, oof.oof_pred.values)\nprint('OVERALL CV Kaggle Metric =',acc)\n\nprint(len(oof))\n\n# CLEAN RAM\ndel train\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T10:48:04.302107Z","iopub.execute_input":"2022-06-30T10:48:04.302676Z","iopub.status.idle":"2022-06-30T11:57:59.562575Z","shell.execute_reply.started":"2022-06-30T10:48:04.302641Z","shell.execute_reply":"2022-06-30T11:57:59.561574Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"oof_xgb = pd.read_parquet(TRAIN_PATH, columns=['customer_ID']).drop_duplicates()\noof_xgb['customer_ID_hash'] = oof_xgb['customer_ID'].apply(lambda x: int(x[-16:],16) ).astype('int64')\noof_xgb = oof_xgb.set_index('customer_ID_hash')\noof_xgb = oof_xgb.merge(oof, left_index=True, right_index=True)\noof_xgb = oof_xgb.sort_index().reset_index(drop=True)\noof_xgb.to_csv(f'oof_xgb_v{VER}.csv',index=False)\noof_xgb.head()\n\n# CLEAR VRAM, RAM FOR INFERENCE BELOW\ndel oof_xgb, oof\n_ = gc.collect()\n\n# CALCULATE SIZE OF EACH SEPARATE TEST PART\ndef get_rows(customers, test, NUM_PARTS = 4, verbose = ''):\n    chunk = len(customers)//NUM_PARTS\n    if verbose != '':\n        print(f'We will process {verbose} data as {NUM_PARTS} separate parts.')\n        print(f'There will be {chunk} customers in each part (except the last part).')\n        print('Below are number of rows in each part:')\n    rows = []\n\n    for k in range(NUM_PARTS):\n        if k==NUM_PARTS-1: cc = customers[k*chunk:]\n        else: cc = customers[k*chunk:(k+1)*chunk]\n        s = test.loc[test.customer_ID.isin(cc)].shape[0]\n        rows.append(s)\n    if verbose != '': print( rows )\n    return rows,chunk\n\n# COMPUTE SIZE OF 4 PARTS FOR TEST DATA\nNUM_PARTS = 4\nTEST_PATH = '../input/amex-data-integer-dtypes-parquet-format/test.parquet'\n\nprint(f'Reading test data...')\ntest = read_file(path = TEST_PATH, usecols = ['customer_ID','S_2'])\ncustomers = test[['customer_ID']].drop_duplicates().sort_index().values.flatten()\nrows,num_cust = get_rows(customers, test[['customer_ID']], NUM_PARTS = NUM_PARTS, verbose = 'test')\n\n\n# INFER TEST DATA IN PARTS\nskip_rows = 0\nskip_cust = 0\ntest_preds = []\n\nfor k in range(NUM_PARTS):\n    \n    # READ PART OF TEST DATA\n    print(f'\\nReading test data...')\n    test = read_file(path = TEST_PATH)\n    test = test.iloc[skip_rows:skip_rows+rows[k]]\n    skip_rows += rows[k]\n    print(f'=> Test part {k+1} has shape', test.shape )\n    \n    # PROCESS AND FEATURE ENGINEER PART OF TEST DATA\n    test = process_and_feature_engineer(test)\n    if k==NUM_PARTS-1: test = test.loc[customers[skip_cust:]]\n    else: test = test.loc[customers[skip_cust:skip_cust+num_cust]]\n    skip_cust += num_cust\n    \n    # TEST DATA FOR XGB\n    X_test = test[FEATURES]\n    dtest = xgb.DMatrix(data=X_test)\n    test = test[['P_2_mean']] # reduce memory\n    del X_test\n    gc.collect()\n\n    # INFER XGB MODELS ON TEST DATA\n    model = xgb.Booster()\n    model.load_model(f'XGB_v{VER}_fold0.xgb')\n    preds = model.predict(dtest) * (fold_model_scores[0] / sum(fold_model_scores))\n    for f in range(1,FOLDS):\n        model.load_model(f'XGB_v{VER}_fold{f}.xgb')\n        preds += model.predict(dtest) * (fold_model_scores[f] / sum(fold_model_scores))\n    \n    test_preds.append(preds)\n\n    # CLEAN MEMORY\n    del dtest, model\n    _ = gc.collect()\n# WRITE SUBMISSION FILE\ntest_preds = np.concatenate(test_preds)   \ntest = cudf.DataFrame(index=customers,data={'prediction':test_preds})","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:57:59.564801Z","iopub.execute_input":"2022-06-30T11:57:59.565178Z","iopub.status.idle":"2022-06-30T12:02:44.566346Z","shell.execute_reply.started":"2022-06-30T11:57:59.565142Z","shell.execute_reply":"2022-06-30T12:02:44.565432Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"sub = cudf.read_csv('../input/amex-default-prediction/sample_submission.csv')[['customer_ID']]\nsub['customer_ID_hash'] = sub['customer_ID'].str[-16:].str.hex_to_int().astype('int64')\nsub = sub.set_index('customer_ID_hash')\nsub = sub.merge(test[['prediction']], left_index=True, right_index=True, how='left')\nsub = sub.reset_index(drop=True)\n\n# DISPLAY PREDICTIONS\nsub.to_csv(f'submission_xgb_v1.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:02:44.567757Z","iopub.execute_input":"2022-06-30T12:02:44.568087Z","iopub.status.idle":"2022-06-30T12:02:45.844364Z","shell.execute_reply.started":"2022-06-30T12:02:44.568054Z","shell.execute_reply":"2022-06-30T12:02:45.842904Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}